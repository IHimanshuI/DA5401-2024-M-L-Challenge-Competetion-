{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Load embeddings and labels\n",
    "embeddings_1 = np.load('embeddings_1.npy')\n",
    "embeddings_2 = np.load('embeddings_2.npy')\n",
    "embeddings = np.vstack([embeddings_1, embeddings_2])  # Combine both embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and convert them to multi-hot encoding\n",
    "with open('icd_codes_1.txt') as f1, open('icd_codes_2.txt') as f2:\n",
    "    labels_1 = [line.strip().split(';') for line in f1]\n",
    "    labels_2 = [line.strip().split(';') for line in f2]\n",
    "    labels = labels_1 + labels_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for ICD10 codes to multi-hot encoding\n",
    "unique_codes = sorted(set(code for sublist in labels for code in sublist))\n",
    "code_to_index = {code: idx for idx, code in enumerate(unique_codes)}\n",
    "num_classes = len(unique_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to multi-hot vectors\n",
    "label_encoded = np.zeros((len(labels), num_classes), dtype=int)\n",
    "for i, label_list in enumerate(labels):\n",
    "    for code in label_list:\n",
    "        label_encoded[i, code_to_index[code]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class HealthRecordDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# Neural Network Architecture\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.15):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience=8):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main(embeddings, label_encoded):\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        embeddings, label_encoded, test_size=0.13, random_state=101\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HealthRecordDataset(X_train, y_train)\n",
    "    val_dataset = HealthRecordDataset(X_val, y_val)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = embeddings.shape[1]\n",
    "    hidden_dims = [512, 256, 128]  \n",
    "    output_dim = label_encoded.shape[1]  \n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiLabelClassifier(input_dim, hidden_dims, output_dim).to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=50,  # Adjust as needed\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 1353/1353 [01:31<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "Training Loss: 0.0051\n",
      "Validation Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 1353/1353 [01:32<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30:\n",
      "Training Loss: 0.0023\n",
      "Validation Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 1353/1353 [01:31<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30:\n",
      "Training Loss: 0.0020\n",
      "Validation Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 1353/1353 [01:34<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30:\n",
      "Training Loss: 0.0019\n",
      "Validation Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 1353/1353 [01:32<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30:\n",
      "Training Loss: 0.0018\n",
      "Validation Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 1353/1353 [01:33<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30:\n",
      "Training Loss: 0.0016\n",
      "Validation Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 1353/1353 [01:34<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30:\n",
      "Training Loss: 0.0015\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 1353/1353 [01:36<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30:\n",
      "Training Loss: 0.0015\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 1353/1353 [01:40<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30:\n",
      "Training Loss: 0.0014\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 1353/1353 [01:38<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30:\n",
      "Training Loss: 0.0013\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 1353/1353 [01:41<00:00, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30:\n",
      "Training Loss: 0.0012\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 1353/1353 [01:41<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30:\n",
      "Training Loss: 0.0012\n",
      "Validation Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 1353/1353 [01:39<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30:\n",
      "Training Loss: 0.0011\n",
      "Validation Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 1353/1353 [01:41<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30:\n",
      "Training Loss: 0.0011\n",
      "Validation Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 1353/1353 [01:41<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30:\n",
      "Training Loss: 0.0010\n",
      "Validation Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 1353/1353 [01:42<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30:\n",
      "Training Loss: 0.0010\n",
      "Validation Loss: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 1353/1353 [01:44<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30:\n",
      "Training Loss: 0.0009\n",
      "Validation Loss: 0.0020\n",
      "Early stopping triggered after 17 epochs\n"
     ]
    }
   ],
   "source": [
    "# Function to make predictions\n",
    "def predict(model, input_data, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.FloatTensor(input_data).to(device)\n",
    "        predictions = model(input_tensor)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main(embeddings, label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (198982, 1400)\n",
      "   A63.0  B07.0  B07.9  B35.1  B37.81\n",
      "0      0      0      0      0       0\n",
      "1      0      0      0      0       0\n",
      "2      0      0      0      0       0\n",
      "3      0      0      0      0       0\n",
      "4      0      0      0      0       0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "\n",
    "data_points = []\n",
    "with open('icd_codes_1.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        labels = [label.strip() for label in line.strip().split(';')]\n",
    "        data_points.append(labels)\n",
    "        \n",
    "with open('icd_codes_2.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        labels = [label.strip() for label in line.strip().split(';')]\n",
    "        data_points.append(labels)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "target_matrix = mlb.fit_transform(data_points)\n",
    "target_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    target_matrix,\n",
    "    columns=mlb.classes_\n",
    ")\n",
    "\n",
    "print(f\"DataFrame shape: {target_df.shape}\")\n",
    "print(target_df.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test embeddings\n",
    "test_embeddings = np.load('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded successfully\n",
      "Test embeddings shape: (99490, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_1000\\3250439566.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (99490, 1400)\n",
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "# Define the model architecture again since we need it to load the state dict\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.15):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Function to make predictions with threshold\n",
    "def predict_with_threshold(model, test_embeddings, threshold=0.5, batch_size=128, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    test_tensor = torch.FloatTensor(test_embeddings)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_tensor), batch_size):\n",
    "            batch = test_tensor[i:i + batch_size].to(device)\n",
    "            batch_preds = model(batch)\n",
    "            # Convert probabilities to binary predictions using threshold\n",
    "            binary_preds = (batch_preds >= threshold).float()\n",
    "            predictions.append(binary_preds.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batch predictions\n",
    "    predictions = np.vstack(predictions)\n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Model parameters (same as training)\n",
    "    input_dim = 1024\n",
    "    hidden_dims = [512, 256, 128]\n",
    "    output_dim = 1400\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiLabelClassifier(input_dim, hidden_dims, output_dim).to(device)\n",
    "    \n",
    "    # Load the saved model\n",
    "    checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded successfully\")\n",
    "\n",
    "    # test embedding shape\n",
    "    print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict_with_threshold(model, test_embeddings, threshold=0.45, device=device)\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    # Convert to dataframe with column names\n",
    "    df_predictions = pd.DataFrame(\n",
    "        predictions,\n",
    "        columns=[f'label_{i}' for i in range(predictions.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_predictions.to_csv('test_predictions.csv', index=False)\n",
    "    print(\"Predictions saved to test_predictions.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file in the specified format\n",
    "# Map indices back to ICD10 codes\n",
    "index_to_code = {v: k for k, v in code_to_index.items()}\n",
    "test_labels_pred = pd.read_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_pred = test_labels_pred.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99490, 1400)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = []\n",
    "for idx, label_vector in enumerate(test_labels_pred, start=1):\n",
    "    # Get codes with predictions above the threshold and sort lexicographically\n",
    "    codes = [index_to_code[i] for i, val in enumerate(label_vector) if val == 1]\n",
    "    codes = sorted(codes)  # Sort lexicographically\n",
    "    label_string = ';'.join(codes) if codes else ''  # Stitch with ';' or leave blank if no label\n",
    "    submission_data.append({'id': idx, 'labels': label_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>G56.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M65.9;S83.242A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>G56.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M65.311;M65.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>S83.241A;S83.281A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99485</th>\n",
       "      <td>99486</td>\n",
       "      <td>K57.30;K63.5;K64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99486</th>\n",
       "      <td>99487</td>\n",
       "      <td>K29.50;K31.89;K90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99487</th>\n",
       "      <td>99488</td>\n",
       "      <td>D12.2;D12.5;K64.8;Z12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99488</th>\n",
       "      <td>99489</td>\n",
       "      <td>B96.81;K21.9;K29.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99489</th>\n",
       "      <td>99490</td>\n",
       "      <td>D12.2;D12.3;K64.0;Z12.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                    labels\n",
       "0          1                    G56.21\n",
       "1          2            M65.9;S83.242A\n",
       "2          3                    G56.01\n",
       "3          4           M65.311;M65.312\n",
       "4          5         S83.241A;S83.281A\n",
       "...      ...                       ...\n",
       "99485  99486        K57.30;K63.5;K64.9\n",
       "99486  99487       K29.50;K31.89;K90.0\n",
       "99487  99488  D12.2;D12.5;K64.8;Z12.11\n",
       "99488  99489       B96.81;K21.9;K29.50\n",
       "99489  99490  D12.2;D12.3;K64.0;Z12.11\n",
       "\n",
       "[99490 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame and save as CSV\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

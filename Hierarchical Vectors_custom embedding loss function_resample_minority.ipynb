{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read and Parse ICD10 Codes from Multiple Files\n",
    "unique_icd10_codes = set()\n",
    "\n",
    "# List of files to read\n",
    "icd_code_files = ['icd_codes_1.txt', 'icd_codes_2.txt']\n",
    "\n",
    "# Read each file and collect unique ICD10 codes\n",
    "for filename in icd_code_files:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            codes = line.strip().split(';')  # Split codes by semicolon\n",
    "            unique_icd10_codes.update(codes)  # Add each code to the set\n",
    "\n",
    "# Convert to a sorted list of unique ICD10 codes\n",
    "icd10_codes = sorted(list(unique_icd10_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Hierarchical Vectors for Each Code\n",
    "# Split ICD10 codes into hierarchical levels\n",
    "primary_categories = [code[0] for code in icd10_codes]      # First character\n",
    "subcategories = [code[1:3] for code in icd10_codes]         # Next two characters\n",
    "suffixes = [code[3:] if len(code) > 3 else '' for code in icd10_codes]  # Remaining part, if any\n",
    "\n",
    "# Combine hierarchical levels into a single array\n",
    "hierarchy_data = np.array([primary_categories, subcategories, suffixes]).T\n",
    "\n",
    "# Encode hierarchy levels into one-hot vectors\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "hierarchical_vectors = encoder.fit_transform(hierarchy_data)\n",
    "\n",
    "# Step 3: Apply PCA to Reduce Dimensions for Label Embeddings\n",
    "pca = PCA(n_components=5)\n",
    "label_embeddings = pca.fit_transform(hierarchical_vectors)\n",
    "\n",
    "# Map each ICD10 code to its embedding\n",
    "code_to_embedding = {code: emb for code, emb in zip(icd10_codes, label_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Load Embedding Data\n",
    "X = np.concatenate([\n",
    "    np.load('embeddings_1.npy'),  # shape should be (samples_1, 1024)\n",
    "    np.load('embeddings_2.npy')   # shape should be (samples_2, 1024)\n",
    "])\n",
    "\n",
    "# Load and Process Labels\n",
    "# Read the ICD10 labels from each file\n",
    "icd_labels = []\n",
    "for filename in ['icd_codes_1.txt', 'icd_codes_2.txt']:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            codes = line.strip().split(';')\n",
    "            icd_labels.append(codes)\n",
    "\n",
    "#Convert Labels to Multi-Hot Encoding\n",
    "# Use MultiLabelBinarizer to encode ICD10 codes\n",
    "mlb = MultiLabelBinarizer(classes=icd10_codes)  \n",
    "y = mlb.fit_transform(icd_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Concatenate X and y along the feature axis to keep X and y together\n",
    "Xy_train = np.hstack((X_train, y_train))\n",
    "\n",
    "# Define the minimum number of samples for each label\n",
    "min_samples = 10\n",
    "\n",
    "for code_index in range(y_train.shape[1]):\n",
    "    # Get samples where this specific label (ICD10 code) is present\n",
    "    minority_samples = Xy_train[Xy_train[:, X_train.shape[1] + code_index] == 1]\n",
    "    \n",
    "    # Check if there are any minority samples and if oversampling is necessary\n",
    "    if len(minority_samples) > 0 and len(minority_samples) < min_samples:\n",
    "        num_to_add = min_samples - len(minority_samples)\n",
    "        \n",
    "        # Perform oversampling with replacement\n",
    "        oversampled_samples = resample(\n",
    "            minority_samples,\n",
    "            replace=True,\n",
    "            n_samples=num_to_add,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Stack the oversampled samples back into the dataset\n",
    "        Xy_train = np.vstack([Xy_train, oversampled_samples])\n",
    "\n",
    "# Split X and y back into separate variables\n",
    "X_train_resampled = Xy_train[:, :X_train.shape[1]]\n",
    "y_train_resampled = Xy_train[:, X_train.shape[1]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom embedding loss function\n",
    "def custom_embedding_loss(y_true, y_pred):\n",
    "    # Cast y_true and y_pred to compatible dtypes\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # For each code in the label embedding, calculate the distance loss\n",
    "    losses = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
    "    \n",
    "    # Take the mean loss across the batch\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define the Three-Layer Model\n",
    "def create_model(input_dim, output_dim, learning_rate=0.0005):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=custom_embedding_loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = 1024  \n",
    "output_dim = len(icd10_codes)  \n",
    "\n",
    "model = create_model(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5952 - loss: 0.3477 - val_accuracy: 0.5682 - val_loss: 0.5939 - learning_rate: 1.0000e-06\n",
      "Epoch 2/5\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.5943 - loss: 0.3475 - val_accuracy: 0.5682 - val_loss: 0.5939 - learning_rate: 1.0000e-06\n",
      "Epoch 3/5\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5921 - loss: 0.3512 - val_accuracy: 0.5683 - val_loss: 0.5940 - learning_rate: 1.0000e-06\n",
      "Epoch 4/5\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5941 - loss: 0.3471 - val_accuracy: 0.5682 - val_loss: 0.5940 - learning_rate: 1.0000e-06\n",
      "Epoch 5/5\n",
      "\u001b[1m1252/1252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5939 - loss: 0.3490 - val_accuracy: 0.5681 - val_loss: 0.5940 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Train the model (ensure X_train and y_train are defined)\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=[reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test embeddings\n",
    "test_embeddings = np.load('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3110/3110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "y_test_pred = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    if y_test_pred[i][1]>=0.4:\n",
    "        print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1244/1244\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Micro F2 Score on validation set: 0.8176\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Predict on the validation set and compute micro F2 score\n",
    "y_val_pred = model.predict(X_val) > 0.45 # Convert probabilities to binary predictions\n",
    "micro_f2_score = f1_score(y_val, y_val_pred, average='micro')\n",
    "\n",
    "print(f'Micro F2 Score on validation set: {micro_f2_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.45\n",
    "test_labels_pred = (y_test_pred > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define code_to_index and index_to_code mappings\n",
    "code_to_index = {code: idx for idx, code in enumerate(icd10_codes)}\n",
    "index_to_code = {idx: code for code, idx in code_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file in the specified format\n",
    "# Map indices back to ICD10 codes\n",
    "index_to_code = {v: k for k, v in code_to_index.items()}\n",
    "\n",
    "submission_data = []\n",
    "for idx, label_vector in enumerate(test_labels_pred, start=1):\n",
    "    # Get codes with predictions above the threshold and sort lexicographically\n",
    "    codes = [index_to_code[i] for i, val in enumerate(label_vector) if val == 1]\n",
    "    codes = sorted(codes)  # Sort lexicographically\n",
    "    label_string = ';'.join(codes) if codes else ''  # Stitch with ';' or leave blank if no label\n",
    "    submission_data.append({'id': idx, 'labels': label_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>G56.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M65.9;S83.242A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>G56.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M65.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>S83.241A;S83.281A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99485</th>\n",
       "      <td>99486</td>\n",
       "      <td>D12.0;D12.5;K57.30;K63.5;K64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99486</th>\n",
       "      <td>99487</td>\n",
       "      <td>K31.89;K90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99487</th>\n",
       "      <td>99488</td>\n",
       "      <td>D12.2;D12.5;K64.8;Z12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99488</th>\n",
       "      <td>99489</td>\n",
       "      <td>B96.81;K21.9;K29.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99489</th>\n",
       "      <td>99490</td>\n",
       "      <td>D12.2;D12.3;K64.0;Z12.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                          labels\n",
       "0          1                          G56.21\n",
       "1          2                  M65.9;S83.242A\n",
       "2          3                          G56.01\n",
       "3          4                         M65.312\n",
       "4          5               S83.241A;S83.281A\n",
       "...      ...                             ...\n",
       "99485  99486  D12.0;D12.5;K57.30;K63.5;K64.9\n",
       "99486  99487                    K31.89;K90.0\n",
       "99487  99488        D12.2;D12.5;K64.8;Z12.11\n",
       "99488  99489             B96.81;K21.9;K29.50\n",
       "99489  99490        D12.2;D12.3;K64.0;Z12.11\n",
       "\n",
       "[99490 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame and save as CSV\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "submission_df.to_csv('hei_submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'hei_submission.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
